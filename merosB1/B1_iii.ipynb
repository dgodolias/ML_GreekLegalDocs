{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0870ee1",
   "metadata": {},
   "source": [
    "# B1.iii - Extreme Gradient Boosting (XGBoost) με Word2Vec\n",
    "\n",
    "Αυτό το notebook υλοποιεί τον αλγόριθμο **XGBoost** (Extreme Gradient Boosting) σε συνδυασμό με **Word2Vec** για την ταξινόμηση νομικών κειμένων. Βασίζεται στον κώδικα από το αρχείο `main_XgBoost.py`.\n",
    "\n",
    "## Περιεχόμενα\n",
    "1. [Τεχνικές Λεπτομέρειες XGBoost](#xgboost-technical)\n",
    "2. [Παράμετροι XGBoost](#xgboost-parameters)\n",
    "3. [Συνδυασμός XGBoost με Word2Vec](#xgboost-word2vec)\n",
    "4. [Υλοποίηση και Εκτέλεση](#implementation)\n",
    "5. [Σύγκριση με άλλους Αλγορίθμους](#comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa15d084",
   "metadata": {},
   "source": [
    "## 1. Τεχνικές Λεπτομέρειες XGBoost {#xgboost-technical}\n",
    "\n",
    "Το **XGBoost** (Extreme Gradient Boosting) είναι μια υλοποίηση του gradient boosting που έχει σχεδιαστεί για ταχύτητα και απόδοση.\n",
    "\n",
    "### Θεωρητικό Υπόβαθρο\n",
    "\n",
    "Το XGBoost βασίζεται στην ιδέα του **ensemble learning**, όπου πολλά αδύναμα μοντέλα (weak learners) συνδυάζονται για να δημιουργήσουν ένα ισχυρό μοντέλο.\n",
    "\n",
    "#### Gradient Boosting Framework\n",
    "\n",
    "Η βασική ιδέα είναι η **sequential training** μοντέλων:\n",
    "\n",
    "$$\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i)$$\n",
    "\n",
    "όπου:\n",
    "- $\\hat{y}_i^{(t)}$ είναι η πρόβλεψη στο βήμα $t$\n",
    "- $f_t(x_i)$ είναι το νέο δέντρο που προστίθεται\n",
    "\n",
    "#### Objective Function\n",
    "\n",
    "Το XGBoost ελαχιστοποιεί την εξής objective function:\n",
    "\n",
    "$$\\mathcal{L}^{(t)} = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) + \\Omega(f_t)$$\n",
    "\n",
    "όπου:\n",
    "- $l$ είναι η loss function\n",
    "- $\\Omega(f_t)$ είναι ο regularization term\n",
    "\n",
    "### Βασικά Χαρακτηριστικά\n",
    "\n",
    "1. **Regularization**: Το XGBoost περιλαμβάνει L1 και L2 regularization\n",
    "2. **Tree Pruning**: Χρησιμοποιεί post-pruning με το max_depth parameter\n",
    "3. **Parallel Processing**: Παραλληλοποίηση της κατασκευής δέντρων\n",
    "4. **Built-in Cross-Validation**: Ενσωματωμένη υποστήριξη για CV\n",
    "5. **Missing Value Handling**: Αυτόματος χειρισμός missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a196a8",
   "metadata": {},
   "source": [
    "## 2. Παράμετροι XGBoost {#xgboost-parameters}\n",
    "\n",
    "### Κύριες Παράμετροι\n",
    "\n",
    "| Παράμετρος | Περιγραφή | Τιμή στο Project |\n",
    "|------------|-----------|------------------|\n",
    "| `n_estimators` | Αριθμός δέντρων (boosting rounds) | 100 |\n",
    "| `learning_rate` | Ρυθμός μάθησης (step size shrinkage) | 0.1 |\n",
    "| `max_depth` | Μέγιστο βάθος δέντρων | 3 |\n",
    "| `eval_metric` | Μετρική αξιολόγησης | 'mlogloss' |\n",
    "| `random_state` | Seed για reproducibility | 42 |\n",
    "\n",
    "### Αναλυτική Περιγραφή Παραμέτρων\n",
    "\n",
    "#### 1. n_estimators (Αριθμός Δέντρων)\n",
    "- **Ρόλος**: Καθορίζει πόσα δέντρα θα κατασκευαστούν\n",
    "- **Επίδραση**: \n",
    "  - Περισσότερα δέντρα → καλύτερη απόδοση, αλλά κίνδυνος overfitting\n",
    "  - Λιγότερα δέντρα → ταχύτερη εκπαίδευση, αλλά κίνδυνος underfitting\n",
    "- **Συνήθεις Τιμές**: 50-1000+\n",
    "\n",
    "#### 2. learning_rate (Ρυθμός Μάθησης)\n",
    "- **Ρόλος**: Ελέγχει τη συνεισφορά κάθε δέντρου\n",
    "- **Μαθηματική Έκφραση**: $\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + \\eta \\cdot f_t(x_i)$\n",
    "- **Επίδραση**:\n",
    "  - Μικρότερη τιμή → πιο σταθερή σύγκλιση, χρειάζεται περισσότερα δέντρα\n",
    "  - Μεγαλύτερη τιμή → ταχύτερη εκπαίδευση, κίνδυνος overshooting\n",
    "- **Συνήθεις Τιμές**: 0.01-0.3\n",
    "\n",
    "#### 3. max_depth (Μέγιστο Βάθος)\n",
    "- **Ρόλος**: Ελέγχει την πολυπλοκότητα κάθε δέντρου\n",
    "- **Επίδραση**:\n",
    "  - Μεγαλύτερο βάθος → πιο πολύπλοκα μοντέλα, κίνδυνος overfitting\n",
    "  - Μικρότερο βάθος → απλούστερα μοντέλα, κίνδυνος underfitting\n",
    "- **Συνήθεις Τιμές**: 3-10\n",
    "\n",
    "#### 4. eval_metric (Μετρική Αξιολόγησης)\n",
    "- **'mlogloss'**: Multi-class logarithmic loss\n",
    "- **Μαθηματική Έκφραση**: $-\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} y_{i,j} \\log(p_{i,j})$\n",
    "- **Χρήση**: Κατάλληλη για πολυκλασικά προβλήματα"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4701eb",
   "metadata": {},
   "source": [
    "## 3. Συνδυασμός XGBoost με Word2Vec {#xgboost-word2vec}\n",
    "\n",
    "### Γιατί XGBoost + Word2Vec;\n",
    "\n",
    "1. **Πλούσια Αναπαράσταση**: Το Word2Vec δημιουργεί dense vectors που κωδικοποιούν σημασιολογικές σχέσεις\n",
    "2. **Non-linear Patterns**: Το XGBoost μπορεί να ανακαλύψει πολύπλοκες σχέσεις στα Word2Vec features\n",
    "3. **Feature Importance**: Το XGBoost παρέχει interpretability μέσω feature importance\n",
    "4. **Robust Performance**: Καλή απόδοση σε διάφορα domains\n",
    "\n",
    "### Διαδικασία Επεξεργασίας\n",
    "\n",
    "```\n",
    "Κείμενο → Tokenization → Word2Vec Training → Document Vectors → XGBoost Training\n",
    "```\n",
    "\n",
    "#### Document Vectorization Strategies\n",
    "\n",
    "1. **Average Pooling**: $\\vec{d} = \\frac{1}{|d|} \\sum_{w \\in d} \\vec{w}$\n",
    "2. **Weighted Average**: $\\vec{d} = \\frac{\\sum_{w \\in d} tf(w) \\cdot \\vec{w}}{\\sum_{w \\in d} tf(w)}$\n",
    "3. **TF-IDF Weighted**: $\\vec{d} = \\frac{\\sum_{w \\in d} tfidf(w) \\cdot \\vec{w}}{\\sum_{w \\in d} tfidf(w)}$\n",
    "\n",
    "### Πλεονεκτήματα του Συνδυασμού\n",
    "\n",
    "| Χαρακτηριστικό | Word2Vec | XGBoost | Συνδυασμός |\n",
    "|----------------|----------|---------|------------|\n",
    "| Σημασιολογική Αναπαράσταση | ✓ | - | ✓ |\n",
    "| Non-linear Learning | - | ✓ | ✓ |\n",
    "| Feature Interactions | - | ✓ | ✓ |\n",
    "| Interpretability | - | ✓ | ✓ |\n",
    "| Regularization | - | ✓ | ✓ |\n",
    "| Scalability | ✓ | ✓ | ✓ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c398e26a",
   "metadata": {},
   "source": [
    "## 4. Υλοποίηση και Εκτέλεση {#implementation}\n",
    "\n",
    "### Βήμα 1: Import Libraries και Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b79fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Εισαγωγή XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(f\"XGBoost version: {xgb.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"XGBoost is not installed. Installing...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"xgboost\"])\n",
    "    import xgboost as xgb\n",
    "    print(f\"XGBoost version: {xgb.__version__}\")\n",
    "\n",
    "# Εισαγωγή Gensim για Word2Vec\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "    print(f\"Gensim available for Word2Vec\")\n",
    "except ImportError:\n",
    "    print(\"Gensim is not installed. Installing...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gensim\"])\n",
    "    from gensim.models import Word2Vec\n",
    "    print(f\"Gensim available for Word2Vec\")\n",
    "\n",
    "# Προσπάθεια εισαγωγής utils\n",
    "try:\n",
    "    from utils import (\n",
    "        load_and_preprocess_data,\n",
    "        run_experiment,\n",
    "        script_execution_timer\n",
    "    )\n",
    "    print(\"Utils module imported successfully\")\n",
    "    UTILS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Utils module not available. Will use simplified implementation.\")\n",
    "    UTILS_AVAILABLE = False\n",
    "\n",
    "# Ρυθμίσεις για matplotlib\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"\\n=== XGBoost με Word2Vec για Ταξινόμηση Νομικών Κειμένων ===\")\n",
    "print(\"Jupyter Notebook Implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad84805",
   "metadata": {},
   "source": [
    "### Βήμα 2: Παράμετροι και Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef7432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ΠΑΡΑΜΕΤΡΟΙ CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Dataset Configuration\n",
    "DATASET_CONFIG = \"subject\"  # available options: \"volume\", \"chapter\", \"subject\"\n",
    "SUBSET_PERCENTAGE = 0.1     # 1.0 for full data, < 1.0 for subset (για γρήγορη εκτέλεση)\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# Word2Vec Parameters\n",
    "WORD2VEC_VECTOR_SIZE = 100  # Διάσταση των word vectors\n",
    "WORD2VEC_WINDOW = 5         # Context window size\n",
    "WORD2VEC_MIN_COUNT = 2      # Minimum word frequency\n",
    "WORD2VEC_WORKERS = 4        # Number of threads\n",
    "WORD2VEC_SG = 0            # 0 = CBOW, 1 = Skip-gram\n",
    "WORD2VEC_EPOCHS = 10       # Training epochs\n",
    "\n",
    "# XGBoost Parameters\n",
    "XGB_N_ESTIMATORS = 100     # Αριθμός δέντρων\n",
    "XGB_LEARNING_RATE = 0.1    # Ρυθμός μάθησης\n",
    "XGB_MAX_DEPTH = 3          # Μέγιστο βάθος δέντρων\n",
    "XGB_EVAL_METRIC = 'mlogloss'  # Μετρική αξιολόγησης\n",
    "\n",
    "print(\"Παράμετροι Configuration:\")\n",
    "print(f\"Dataset: {DATASET_CONFIG}\")\n",
    "print(f\"Subset percentage: {SUBSET_PERCENTAGE*100:.0f}%\")\n",
    "print(f\"Word2Vec vector size: {WORD2VEC_VECTOR_SIZE}\")\n",
    "print(f\"Word2Vec algorithm: {'CBOW' if WORD2VEC_SG == 0 else 'Skip-gram'}\")\n",
    "print(f\"XGBoost estimators: {XGB_N_ESTIMATORS}\")\n",
    "print(f\"XGBoost learning rate: {XGB_LEARNING_RATE}\")\n",
    "print(f\"XGBoost max depth: {XGB_MAX_DEPTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9a3dad",
   "metadata": {},
   "source": [
    "### Βήμα 3: Δημιουργία Απλουστευμένων Συναρτήσεων (Fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd58ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SIMPLIFIED FUNCTIONS (αν δεν είναι διαθέσιμο το utils module)\n",
    "# =============================================================================\n",
    "\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"Απλή tokenization για ελληνικά κείμενα\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    # Μετατροπή σε πεζά\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Αφαίρεση σημείων στίξης\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Διαχωρισμός σε λέξεις\n",
    "    words = text.split()\n",
    "    \n",
    "    # Φιλτράρισμα κενών strings\n",
    "    words = [word for word in words if word.strip()]\n",
    "    \n",
    "    return words\n",
    "\n",
    "def create_sample_data():\n",
    "    \"\"\"Δημιουργία sample δεδομένων για testing\"\"\"\n",
    "    print(\"Creating sample Greek legal documents for testing...\")\n",
    "    \n",
    "    # Sample ελληνικά νομικά κείμενα\n",
    "    sample_texts = [\n",
    "        \"Το άρθρο 1 του Συντάγματος ορίζει την Ελλάδα ως δημοκρατικό κράτος\",\n",
    "        \"Σύμφωνα με τον Αστικό Κώδικα η συμφωνία δεσμεύει τα συμβαλλόμενα μέρη\",\n",
    "        \"Ο Ποινικός Κώδικας προβλέπει ποινές για τα αδικήματα κατά της ζωής\",\n",
    "        \"Το δικαστήριο αποφάσισε για την εφαρμογή των διατάξεων του νόμου\",\n",
    "        \"Η Ευρωπαϊκή Σύμβαση Δικαιωμάτων του Ανθρώπου κατοχυρώνει βασικά δικαιώματα\",\n",
    "        \"Το άρθρο 8 της ΕΣΔΑ προστατεύει το δικαίωμα στην ιδιωτική ζωή\",\n",
    "        \"Η διαδικασία του άρθρου 867 του ΚΠολΔ αφορά την αναγνώριση αποφάσεων\",\n",
    "        \"Σύμφωνα με το άρθρο 281 ΑΚ η αποζημίωση καλύπτει τη θετική και αρνητική ζημία\"\n",
    "    ]\n",
    "    \n",
    "    # Sample labels\n",
    "    sample_labels = [\n",
    "        \"Συνταγματικό Δίκαιο\",\n",
    "        \"Αστικό Δίκαιο\", \n",
    "        \"Ποινικό Δίκαιο\",\n",
    "        \"Δικονομία\",\n",
    "        \"Ευρωπαϊκό Δίκαιο\",\n",
    "        \"Ευρωπαϊκό Δίκαιο\",\n",
    "        \"Δικονομία\",\n",
    "        \"Αστικό Δίκαιο\"\n",
    "    ]\n",
    "    \n",
    "    # Δημιουργία περισσότερων samples με variations\n",
    "    extended_texts = []\n",
    "    extended_labels = []\n",
    "    \n",
    "    for i in range(20):  # Δημιουργία 20 samples\n",
    "        idx = i % len(sample_texts)\n",
    "        text = sample_texts[idx]\n",
    "        label = sample_labels[idx]\n",
    "        \n",
    "        # Προσθήκη μικρών variations\n",
    "        if i >= len(sample_texts):\n",
    "            text = f\"{text} Επιπλέον διατάξεις του νόμου αριθμός {i}.\"\n",
    "        \n",
    "        extended_texts.append(text)\n",
    "        extended_labels.append(label)\n",
    "    \n",
    "    return extended_texts, extended_labels\n",
    "\n",
    "def document_to_vector(doc_words, w2v_model, vector_size):\n",
    "    \"\"\"Μετατροπή εγγράφου σε vector χρησιμοποιώντας Word2Vec\"\"\"\n",
    "    vectors = []\n",
    "    for word in doc_words:\n",
    "        if word in w2v_model.wv:\n",
    "            vectors.append(w2v_model.wv[word])\n",
    "    \n",
    "    if vectors:\n",
    "        # Average pooling\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        # Return zero vector if no words found\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "print(\"Simplified functions created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b485ea97",
   "metadata": {},
   "source": [
    "### Βήμα 4: Φόρτωση και Προετοιμασία Δεδομένων"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c991e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ΦΟΡΤΩΣΗ ΚΑΙ ΠΡΟΕΤΟΙΜΑΣΙΑ ΔΕΔΟΜΕΝΩΝ\n",
    "# =============================================================================\n",
    "\n",
    "if UTILS_AVAILABLE:\n",
    "    print(\"Loading data using utils module...\")\n",
    "    try:\n",
    "        texts_proc, labels_proc, unique_labels_proc, unique_labels_proc_str = load_and_preprocess_data(\n",
    "            DATASET_CONFIG, SUBSET_PERCENTAGE, RANDOM_STATE\n",
    "        )\n",
    "        \n",
    "        if texts_proc is None or len(texts_proc) == 0:\n",
    "            print(\"Failed to load data from utils. Using sample data.\")\n",
    "            texts_proc, labels_str = create_sample_data()\n",
    "            # Convert labels to numeric\n",
    "            unique_labels_str = list(set(labels_str))\n",
    "            label_to_idx = {label: idx for idx, label in enumerate(unique_labels_str)}\n",
    "            labels_proc = [label_to_idx[label] for label in labels_str]\n",
    "            unique_labels_proc = list(range(len(unique_labels_str)))\n",
    "            unique_labels_proc_str = unique_labels_str\n",
    "        else:\n",
    "            print(f\"Successfully loaded {len(texts_proc)} documents\")\n",
    "            print(f\"Number of unique labels: {len(unique_labels_proc)}\")\n",
    "            print(f\"Labels: {unique_labels_proc_str}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data with utils: {e}\")\n",
    "        print(\"Using sample data instead.\")\n",
    "        texts_proc, labels_str = create_sample_data()\n",
    "        unique_labels_str = list(set(labels_str))\n",
    "        label_to_idx = {label: idx for idx, label in enumerate(unique_labels_str)}\n",
    "        labels_proc = [label_to_idx[label] for label in labels_str]\n",
    "        unique_labels_proc = list(range(len(unique_labels_str)))\n",
    "        unique_labels_proc_str = unique_labels_str\n",
    "else:\n",
    "    print(\"Utils not available. Using sample data...\")\n",
    "    texts_proc, labels_str = create_sample_data()\n",
    "    \n",
    "    # Convert string labels to numeric\n",
    "    unique_labels_str = list(set(labels_str))\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(unique_labels_str)}\n",
    "    labels_proc = [label_to_idx[label] for label in labels_str]\n",
    "    unique_labels_proc = list(range(len(unique_labels_str)))\n",
    "    unique_labels_proc_str = unique_labels_str\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"Number of documents: {len(texts_proc)}\")\n",
    "print(f\"Number of classes: {len(unique_labels_proc)}\")\n",
    "print(f\"Class names: {unique_labels_proc_str}\")\n",
    "\n",
    "# Έλεγχος κατανομής κλάσεων\n",
    "label_counts = Counter(labels_proc)\n",
    "print(f\"\\nClass distribution:\")\n",
    "for label_idx, count in label_counts.items():\n",
    "    label_name = unique_labels_proc_str[label_idx]\n",
    "    print(f\"  {label_name}: {count} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880cd20e",
   "metadata": {},
   "source": [
    "### Βήμα 5: Tokenization και Word2Vec Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174f4051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TOKENIZATION ΚΑΙ WORD2VEC TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Starting tokenization...\")\n",
    "\n",
    "# Tokenization των κειμένων\n",
    "tokenized_texts = []\n",
    "for text in texts_proc:\n",
    "    tokens = simple_tokenize(text)\n",
    "    tokenized_texts.append(tokens)\n",
    "\n",
    "print(f\"Tokenized {len(tokenized_texts)} documents\")\n",
    "\n",
    "# Εμφάνιση στατιστικών tokenization\n",
    "token_counts = [len(tokens) for tokens in tokenized_texts]\n",
    "print(f\"Average tokens per document: {np.mean(token_counts):.1f}\")\n",
    "print(f\"Min tokens: {np.min(token_counts)}\")\n",
    "print(f\"Max tokens: {np.max(token_counts)}\")\n",
    "\n",
    "# Δείγμα tokenized text\n",
    "print(f\"\\nSample tokenized text:\")\n",
    "print(f\"Original: {texts_proc[0][:100]}...\")\n",
    "print(f\"Tokenized: {tokenized_texts[0][:15]}...\")\n",
    "\n",
    "print(\"\\nTraining Word2Vec model...\")\n",
    "\n",
    "# Εκπαίδευση Word2Vec μοντέλου\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_texts,\n",
    "    vector_size=WORD2VEC_VECTOR_SIZE,\n",
    "    window=WORD2VEC_WINDOW,\n",
    "    min_count=WORD2VEC_MIN_COUNT,\n",
    "    workers=WORD2VEC_WORKERS,\n",
    "    sg=WORD2VEC_SG,\n",
    "    epochs=WORD2VEC_EPOCHS,\n",
    "    seed=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Word2Vec model trained successfully!\")\n",
    "print(f\"Vocabulary size: {len(w2v_model.wv)}\")\n",
    "print(f\"Vector size: {w2v_model.vector_size}\")\n",
    "\n",
    "# Εμφάνιση sample words από το vocabulary\n",
    "vocab_words = list(w2v_model.wv.index_to_key)[:10]\n",
    "print(f\"Sample vocabulary words: {vocab_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d8e79f",
   "metadata": {},
   "source": [
    "### Βήμα 6: Δημιουργία Document Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52a94e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ΔΗΜΙΟΥΡΓΙΑ DOCUMENT VECTORS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Converting documents to vectors...\")\n",
    "\n",
    "# Μετατροπή κάθε εγγράφου σε vector\n",
    "document_vectors = []\n",
    "words_found_count = 0\n",
    "total_words = 0\n",
    "\n",
    "for doc_tokens in tokenized_texts:\n",
    "    doc_vector = document_to_vector(doc_tokens, w2v_model, WORD2VEC_VECTOR_SIZE)\n",
    "    document_vectors.append(doc_vector)\n",
    "    \n",
    "    # Στατιστικά για coverage\n",
    "    found_in_vocab = sum(1 for word in doc_tokens if word in w2v_model.wv)\n",
    "    words_found_count += found_in_vocab\n",
    "    total_words += len(doc_tokens)\n",
    "\n",
    "# Μετατροπή σε numpy array\n",
    "X = np.array(document_vectors)\n",
    "y = np.array(labels_proc)\n",
    "\n",
    "print(f\"Document vectors created successfully!\")\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Vocabulary coverage: {words_found_count/total_words*100:.1f}%\")\n",
    "\n",
    "# Έλεγχος για NaN values\n",
    "nan_count = np.isnan(X).sum()\n",
    "if nan_count > 0:\n",
    "    print(f\"Warning: Found {nan_count} NaN values. Replacing with zeros.\")\n",
    "    X = np.nan_to_num(X)\n",
    "\n",
    "print(f\"Feature vector statistics:\")\n",
    "print(f\"Mean: {X.mean():.4f}\")\n",
    "print(f\"Std: {X.std():.4f}\")\n",
    "print(f\"Min: {X.min():.4f}\")\n",
    "print(f\"Max: {X.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509eee76",
   "metadata": {},
   "source": [
    "### Βήμα 7: Train/Test Split και XGBoost Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAIN/TEST SPLIT ΚΑΙ XGBOOST TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Splitting data into train/test sets...\")\n",
    "\n",
    "# Διαχωρισμός σε train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "print(\"\\nTraining XGBoost model...\")\n",
    "\n",
    "# Δημιουργία και εκπαίδευση XGBoost μοντέλου\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=XGB_N_ESTIMATORS,\n",
    "    learning_rate=XGB_LEARNING_RATE,\n",
    "    max_depth=XGB_MAX_DEPTH,\n",
    "    eval_metric=XGB_EVAL_METRIC,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbosity=1  # Μειωμένη verbosity για cleaner output\n",
    ")\n",
    "\n",
    "# Εκπαίδευση μοντέλου\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"XGBoost model trained successfully!\")\n",
    "\n",
    "# Προβλέψεις\n",
    "print(\"Making predictions...\")\n",
    "y_pred_train = xgb_model.predict(X_train)\n",
    "y_pred_test = xgb_model.predict(X_test)\n",
    "\n",
    "# Υπολογισμός accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Overfitting Gap: {train_accuracy - test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617cd14b",
   "metadata": {},
   "source": [
    "### Βήμα 8: Αναλυτική Αξιολόγηση και Οπτικοποιήσεις"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e73346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ΑΝΑΛΥΤΙΚΗ ΑΞΙΟΛΟΓΗΣΗ\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== Classification Report ===\")\n",
    "report = classification_report(\n",
    "    y_test, y_pred_test, \n",
    "    target_names=unique_labels_proc_str,\n",
    "    digits=4\n",
    ")\n",
    "print(report)\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\n=== Confusion Matrix ===\")\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(cm)\n",
    "\n",
    "# Οπτικοποίηση Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=unique_labels_proc_str,\n",
    "    yticklabels=unique_labels_proc_str\n",
    ")\n",
    "plt.title('Confusion Matrix - XGBoost με Word2Vec')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance\n",
    "print(\"\\n=== Feature Importance Analysis ===\")\n",
    "feature_importance = xgb_model.feature_importances_\n",
    "\n",
    "# Top 20 σημαντικότερα features\n",
    "top_features = np.argsort(feature_importance)[-20:]\n",
    "top_importance = feature_importance[top_features]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(top_features)), top_importance)\n",
    "plt.yticks(range(len(top_features)), [f'Feature {i}' for i in top_features])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Feature Importance - XGBoost')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average feature importance: {feature_importance.mean():.6f}\")\n",
    "print(f\"Max feature importance: {feature_importance.max():.6f}\")\n",
    "print(f\"Min feature importance: {feature_importance.min():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44634b2",
   "metadata": {},
   "source": [
    "### Βήμα 9: Παραμετρική Ανάλυση και Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6116ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ΠΑΡΑΜΕΤΡΙΚΗ ΑΝΑΛΥΣΗ ΚΑΙ INSIGHTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== XGBoost Model Information ===\")\n",
    "print(f\"Number of estimators: {xgb_model.n_estimators}\")\n",
    "print(f\"Learning rate: {xgb_model.learning_rate}\")\n",
    "print(f\"Max depth: {xgb_model.max_depth}\")\n",
    "print(f\"Number of features: {xgb_model.n_features_in_}\")\n",
    "print(f\"Number of classes: {xgb_model.n_classes_}\")\n",
    "\n",
    "print(\"\\n=== Word2Vec Model Information ===\")\n",
    "print(f\"Vector size: {w2v_model.vector_size}\")\n",
    "print(f\"Window size: {w2v_model.window}\")\n",
    "print(f\"Min count: {w2v_model.min_count}\")\n",
    "print(f\"Algorithm: {'CBOW' if w2v_model.sg == 0 else 'Skip-gram'}\")\n",
    "print(f\"Training epochs: {w2v_model.epochs}\")\n",
    "print(f\"Vocabulary size: {len(w2v_model.wv)}\")\n",
    "\n",
    "# Ανάλυση predictions per class\n",
    "print(\"\\n=== Predictions per Class ===\")\n",
    "from sklearn.metrics import classification_report\n",
    "report_dict = classification_report(\n",
    "    y_test, y_pred_test, \n",
    "    target_names=unique_labels_proc_str,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "for class_name in unique_labels_proc_str:\n",
    "    metrics = report_dict[class_name]\n",
    "    print(f\"{class_name}:\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-score: {metrics['f1-score']:.4f}\")\n",
    "    print(f\"  Support: {int(metrics['support'])}\")\n",
    "\n",
    "# Macro και Weighted averages\n",
    "print(f\"\\nMacro avg F1-score: {report_dict['macro avg']['f1-score']:.4f}\")\n",
    "print(f\"Weighted avg F1-score: {report_dict['weighted avg']['f1-score']:.4f}\")\n",
    "\n",
    "# Performance Visualization\n",
    "classes = unique_labels_proc_str\n",
    "precision_scores = [report_dict[cls]['precision'] for cls in classes]\n",
    "recall_scores = [report_dict[cls]['recall'] for cls in classes]\n",
    "f1_scores = [report_dict[cls]['f1-score'] for cls in classes]\n",
    "\n",
    "x_pos = np.arange(len(classes))\n",
    "width = 0.25\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x_pos - width, precision_scores, width, label='Precision', alpha=0.8)\n",
    "plt.bar(x_pos, recall_scores, width, label='Recall', alpha=0.8)\n",
    "plt.bar(x_pos + width, f1_scores, width, label='F1-score', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Performance Metrics per Class - XGBoost με Word2Vec')\n",
    "plt.xticks(x_pos, classes, rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d94d43",
   "metadata": {},
   "source": [
    "## 5. Σύγκριση με άλλους Αλγορίθμους {#comparison}\n",
    "\n",
    "### Σύγκριση XGBoost vs άλλα μοντέλα\n",
    "\n",
    "| Αλγόριθμος | Πλεονεκτήματα | Μειονεκτήματα | Κατάλληλος για |\n",
    "|------------|---------------|---------------|----------------|\n",
    "| **XGBoost** | • Υψηλή απόδοση<br>• Feature importance<br>• Regularization<br>• Χειρισμός missing values | • Πολυπλοκότητα παραμέτρων<br>• Χρόνος εκπαίδευσης | Structured data, competitions |\n",
    "| **SVM** | • Solid theoretical foundation<br>• Kernel trick<br>• Good with high-dim data | • Slow σε μεγάλα datasets<br>• Sensitive σε scaling | High-dimensional, small datasets |\n",
    "| **Logistic Regression** | • Interpretable<br>• Fast training<br>• Probabilistic output | • Linear assumptions<br>• Limited expressiveness | Linear separable data, baselines |\n",
    "\n",
    "### Συνδυασμός με Word2Vec Features\n",
    "\n",
    "| Χαρακτηριστικό | SVM + BoW/TF-IDF | LogReg + Word2Vec | XGBoost + Word2Vec |\n",
    "|----------------|------------------|-------------------|-----------|\n",
    "| **Semantic Understanding** | ❌ | ✅ | ✅ |\n",
    "| **Feature Interactions** | ❌ | ❌ | ✅ |\n",
    "| **Non-linearity** | ✅ (με kernels) | ❌ | ✅ |\n",
    "| **Interpretability** | ⚠️ | ✅ | ⚠️ |\n",
    "| **Training Speed** | ⚠️ | ✅ | ⚠️ |\n",
    "| **Memory Usage** | ⚠️ | ✅ | ⚠️ |\n",
    "\n",
    "### Τι κάνει τον XGBoost + Word2Vec αποτελεσματικό;\n",
    "\n",
    "1. **Rich Feature Representation**: Το Word2Vec δημιουργεί πλούσια semantic features\n",
    "2. **Non-linear Learning**: Το XGBoost μπορεί να μάθει πολύπλοκες σχέσεις\n",
    "3. **Feature Interactions**: Αυτόματη ανακάλυψη interactions μεταξύ features\n",
    "4. **Regularization**: Αποφυγή overfitting με built-in regularization\n",
    "5. **Robustness**: Καλή απόδοση σε διάφορα domains και datasets\n",
    "\n",
    "### Πρακτικές Συμβουλές\n",
    "\n",
    "#### Για XGBoost:\n",
    "- Ξεκινήστε με `learning_rate=0.1` και `max_depth=3-6`\n",
    "- Χρησιμοποιήστε early stopping με validation set\n",
    "- Δοκιμάστε διαφορετικές τιμές για `n_estimators`\n",
    "- Ελέγξτε το feature importance για insights\n",
    "\n",
    "#### Για Word2Vec:\n",
    "- `vector_size=100-300` για τα περισσότερα tasks\n",
    "- `window=5-10` ανάλογα με το domain\n",
    "- Χρησιμοποιήστε CBOW για μικρά datasets, Skip-gram για μεγάλα\n",
    "- Ελέγξτε το vocabulary coverage\n",
    "\n",
    "#### Για Document Vectorization:\n",
    "- Average pooling είναι απλό και αποτελεσματικό\n",
    "- Δοκιμάστε TF-IDF weighted averaging\n",
    "- Εξετάστε την προσθήκη statistical features (length, avg word freq, κλπ.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a49f142",
   "metadata": {},
   "source": [
    "## Συμπεράσματα\n",
    "\n",
    "Το notebook αυτό παρουσίασε την υλοποίηση του **XGBoost με Word2Vec** για ταξινόμηση νομικών κειμένων. Τα κύρια σημεία:\n",
    "\n",
    "### Τεχνικά Highlights:\n",
    "1. **XGBoost Algorithm**: Gradient boosting με regularization και feature importance\n",
    "2. **Word2Vec Integration**: Semantic word embeddings για πλούσια αναπαράσταση\n",
    "3. **Document Vectorization**: Average pooling για μετατροπή σε document-level features\n",
    "4. **Parameter Optimization**: Επεξήγηση και tuning των κύριων παραμέτρων\n",
    "\n",
    "### Πλεονεκτήματα της Προσέγγισης:\n",
    "- **Semantic Understanding**: Κατανόηση σημασιολογικών σχέσεων μέσω Word2Vec\n",
    "- **Non-linear Learning**: Ανακάλυψη πολύπλοκων patterns με XGBoost\n",
    "- **Feature Interpretability**: Feature importance analysis\n",
    "- **Robustness**: Καλή απόδοση σε διάφορα domains\n",
    "\n",
    "### Προτάσεις για Βελτίωση:\n",
    "1. **Hyperparameter Tuning**: Grid search ή Bayesian optimization\n",
    "2. **Feature Engineering**: Προσθήκη statistical ή domain-specific features\n",
    "3. **Ensemble Methods**: Συνδυασμός με άλλα μοντέλα\n",
    "4. **Cross-Validation**: Πιο robust αξιολόγηση με K-fold CV\n",
    "\n",
    "Η συνδυασμένη χρήση XGBoost + Word2Vec αποτελεί μια ισχυρή προσέγγιση για text classification που συνδυάζει την σημασιολογική αναπαράσταση του Word2Vec με την ικανότητα του XGBoost να μαθαίνει πολύπλοκες σχέσεις."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
