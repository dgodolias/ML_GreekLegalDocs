{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d09ce41",
   "metadata": {},
   "source": [
    "# Β1.ii) Ταξινόμηση με Logistic Regression και Word2Vec\n",
    "\n",
    "Αυτό το notebook εκτελεί ταξινόμηση με Logistic Regression χρησιμοποιώντας Word2Vec για την εξαγωγή χαρακτηριστικών από κείμενο.\n",
    "\n",
    "**Κύρια Χαρακτηριστικά:**\n",
    "- **Μοντέλο**: Logistic Regression με liblinear solver\n",
    "- **Feature Extraction**: Word2Vec embeddings με document-level aggregation\n",
    "- **Dataset**: \"greek_legal_code\" από το Hugging Face Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d72efc3",
   "metadata": {},
   "source": [
    "## Τεχνικές Λεπτομέρειες Logistic Regression\n",
    "\n",
    "### Logistic Regression Παράμετροι:\n",
    "\n",
    "**Solver**: `liblinear`\n",
    "- Κατάλληλος για μικρά προς μεσαία datasets\n",
    "- Υποστηρίζει L1 και L2 regularization\n",
    "- Αποδοτικός για binary και multiclass classification\n",
    "\n",
    "**Max Iterations**: `1000`\n",
    "- Μέγιστος αριθμός επαναλήψεων για convergence\n",
    "- Εξασφαλίζει ότι ο αλγόριθμος θα τερματίσει\n",
    "\n",
    "**Random State**: `42`\n",
    "- Εξασφαλίζει αναπαραγωγιμότητα των αποτελεσμάτων\n",
    "\n",
    "### Μαθηματικό Υπόβαθρο:\n",
    "\n",
    "**Logistic Function:**\n",
    "- P(y=1|x) = 1 / (1 + e^(-z))\n",
    "- όπου z = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ\n",
    "\n",
    "**Cost Function (Log-Likelihood):**\n",
    "- J(θ) = -1/m * Σ[y·log(h(x)) + (1-y)·log(1-h(x))]\n",
    "\n",
    "**Πλεονεκτήματα:**\n",
    "- Γρήγορη εκπαίδευση και πρόβλεψη\n",
    "- Παρέχει πιθανότητες κλάσεων\n",
    "- Δεν απαιτεί feature scaling (αλλά βοηθάει)\n",
    "- Ερμηνεύσιμα coefficients\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f192cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from datasets import load_dataset\n",
    "from gensim.models import Word2Vec\n",
    "import json\n",
    "\n",
    "# Προσθήκη του parent directory στο path για να μπορούμε να εισάγουμε το utils\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Import utils functions\n",
    "try:\n",
    "    from utils import (\n",
    "        load_and_preprocess_data,\n",
    "        run_experiment,\n",
    "        script_execution_timer\n",
    "    )\n",
    "    print(\"Utils imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing utils: {e}\")\n",
    "    print(\"Continuing without utils functions...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15be1cd",
   "metadata": {},
   "source": [
    "## Παραμετροποίηση\n",
    "\n",
    "Ορισμός των παραμέτρων για το πείραμα Logistic Regression με Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fcde83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Κοινές παράμετροι\n",
    "DATASET_CONFIG = \"subject\"  # επιλογές: \"volume\", \"chapter\", \"subject\"\n",
    "SUBSET_PERCENTAGE = 1.0     # 1.0 για όλα τα δεδομένα, <1.0 για υποσύνολο\n",
    "N_SPLITS_CV = 1             # 1 για single split, >1 για K-Fold CV\n",
    "RANDOM_STATE = 42\n",
    "SINGLE_SPLIT_TEST_SIZE = 0.2\n",
    "\n",
    "# Feature Engineering\n",
    "SAVE_TRAINED_FEATURE_MODELS = True\n",
    "LOAD_TRAINED_FEATURE_MODELS_IF_EXIST = True\n",
    "\n",
    "# Word2Vec παράμετροι\n",
    "WORD2VEC_VECTOR_SIZE = 100   # Διάσταση των word embeddings\n",
    "WORD2VEC_WINDOW = 5          # Μέγεθος παραθύρου context\n",
    "WORD2VEC_MIN_COUNT = 2       # Ελάχιστη συχνότητα λέξης\n",
    "WORD2VEC_WORKERS = 4         # Αριθμός CPU cores\n",
    "WORD2VEC_SG = 0              # 0 για CBOW, 1 για Skip-gram\n",
    "WORD2VEC_EPOCHS = 10         # Εποχές εκπαίδευσης\n",
    "\n",
    "print(f\"Dataset config: {DATASET_CONFIG}\")\n",
    "print(f\"Subset percentage: {SUBSET_PERCENTAGE}\")\n",
    "print(f\"Word2Vec vector size: {WORD2VEC_VECTOR_SIZE}\")\n",
    "print(f\"Word2Vec window: {WORD2VEC_WINDOW}\")\n",
    "print(f\"Word2Vec algorithm: {'Skip-gram' if WORD2VEC_SG else 'CBOW'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf806291",
   "metadata": {},
   "source": [
    "## Τεχνικά Χαρακτηριστικά Word2Vec\n",
    "\n",
    "### Word2Vec Architecture:\n",
    "\n",
    "**CBOW (Continuous Bag of Words)** - Προεπιλογή:\n",
    "- **Πρόβλεψη**: Προβλέπει την κεντρική λέξη από το context\n",
    "- **Αρχιτεκτονική**: Input → Hidden Layer → Output\n",
    "- **Ταχύτητα**: Γρηγορότερη εκπαίδευση\n",
    "- **Απόδοση**: Καλύτερη σε συχνές λέξεις\n",
    "\n",
    "**Skip-gram** - Εναλλακτική:\n",
    "- **Πρόβλεψη**: Προβλέπει το context από την κεντρική λέξη  \n",
    "- **Αρχιτεκτονική**: Input → Hidden Layer → Multiple Outputs\n",
    "- **Ταχύτητα**: Βραδύτερη εκπαίδευση\n",
    "- **Απόδοση**: Καλύτερη σε σπάνιες λέξεις και μικρά datasets\n",
    "\n",
    "### Παράμετροι Word2Vec:\n",
    "\n",
    "- **Vector Size (100)**: Διάσταση του embedding space\n",
    "- **Window (5)**: Context window - λέξεις πριν και μετά την target\n",
    "- **Min Count (2)**: Φιλτράρισμα σπάνιων λέξεων (<2 εμφανίσεις)\n",
    "- **Workers (4)**: Παραλληλοποίηση για ταχύτητα\n",
    "- **Epochs (10)**: Πολλαπλά περάσματα από το corpus\n",
    "\n",
    "### Document-Level Feature Extraction:\n",
    "\n",
    "**Aggregation Methods:**\n",
    "1. **Mean Pooling**: Μέσος όρος όλων των word vectors (συνήθης επιλογή)\n",
    "2. **Max Pooling**: Μέγιστη τιμή ανά διάσταση\n",
    "3. **Weighted Average**: Σταθμισμένος μέσος (π.χ. με TF-IDF weights)\n",
    "\n",
    "**Πλεονεκτήματα Word2Vec:**\n",
    "- Καταγράφει σημασιολογικές σχέσεις μεταξύ λέξεων\n",
    "- Dense representations (αντί για sparse BoW/TF-IDF)\n",
    "- Χαμηλότερη διάσταση από vocabulary-based methods\n",
    "- Μπορεί να γενικεύσει σε νέες λέξεις με παρόμοια contexts\n",
    "\n",
    "**Μειονεκτήματα:**\n",
    "- Χρειάζεται αρκετά δεδομένα για καλή εκπαίδευση\n",
    "- Δεν κατανοεί τη σειρά των λέξεων στο document\n",
    "- Aggregation μπορεί να χάσει σημαντική πληροφορία"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22b49ea",
   "metadata": {},
   "source": [
    "## Φόρτωση και Προεπεξεργασία Δεδομένων\n",
    "\n",
    "Φόρτωση του dataset και προετοιμασία για το πείραμα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b8219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Φόρτωση των δεδομένων\n",
    "print(\"Φόρτωση δεδομένων...\")\n",
    "\n",
    "# Αν υπάρχει η συνάρτηση utils, χρησιμοποιούμε αυτή\n",
    "if 'load_and_preprocess_data' in globals():\n",
    "    all_texts, all_labels, label_names, num_classes = load_and_preprocess_data(\n",
    "        DATASET_CONFIG, SUBSET_PERCENTAGE, RANDOM_STATE\n",
    "    )\n",
    "else:\n",
    "    # Εναλλακτική φόρτωση χωρίς utils\n",
    "    print(\"Φόρτωση dataset χωρίς utils...\")\n",
    "    try:\n",
    "        ds = load_dataset(\"AI-team-UoA/greek_legal_code\", DATASET_CONFIG, trust_remote_code=True)\n",
    "        dataset_split = ds['train']\n",
    "        all_texts = dataset_split['text']\n",
    "        all_labels = np.array(dataset_split['label'])\n",
    "        label_names = dataset_split.features['label'].names\n",
    "        num_classes = len(label_names)\n",
    "        print(f\"Dataset loaded successfully. Samples: {len(all_texts)}, Classes: {num_classes}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        all_texts, all_labels, label_names, num_classes = None, None, None, None\n",
    "\n",
    "if all_texts is not None:\n",
    "    print(f\"Συνολικά samples: {len(all_texts)}\")\n",
    "    print(f\"Αριθμός κλάσεων: {num_classes}\")\n",
    "    print(f\"Ονόματα κλάσεων: {label_names}\")\n",
    "    print(f\"Παράδειγμα κειμένου: {all_texts[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344858c5",
   "metadata": {},
   "source": [
    "## Logistic Regression με Word2Vec\n",
    "\n",
    "### Διαδικασία Feature Extraction:\n",
    "\n",
    "1. **Tokenization**: Διαχωρισμός κειμένων σε λέξεις\n",
    "2. **Word2Vec Training**: Εκπαίδευση embeddings στο corpus\n",
    "3. **Document Vectorization**: Μετατροπή κάθε εγγράφου σε vector μέσω mean pooling\n",
    "4. **Classification**: Εκπαίδευση Logistic Regression στα document vectors\n",
    "\n",
    "Εκτέλεση ταξινόμησης με Logistic Regression χρησιμοποιώντας Word2Vec για την εξαγωγή χαρακτηριστικών."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31815910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logistic_regression_word2vec_experiment():\n",
    "    \"\"\"Εκτέλεση Logistic Regression με Word2Vec\"\"\"\n",
    "    \n",
    "    print(\"=== Ξεκινάει το πείραμα Logistic Regression με Word2Vec ===\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Παραμετροποίηση\n",
    "    model_name_script = \"LogRegr\"\n",
    "    feature_method_name = \"Word2Vec\"\n",
    "    base_run_id = f\"{model_name_script}_{feature_method_name}_{DATASET_CONFIG}\"\n",
    "    \n",
    "    # Ρύθμιση feature extraction\n",
    "    feature_config = {\n",
    "        'method': 'word2vec',\n",
    "        'params': {\n",
    "            'vector_size': WORD2VEC_VECTOR_SIZE,\n",
    "            'window': WORD2VEC_WINDOW,\n",
    "            'min_count': WORD2VEC_MIN_COUNT,\n",
    "            'workers': WORD2VEC_WORKERS,\n",
    "            'sg': WORD2VEC_SG,\n",
    "            'epochs': WORD2VEC_EPOCHS\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Ρύθμιση μοντέλου\n",
    "    model_class = LogisticRegression\n",
    "    model_init_params = {\n",
    "        'solver': 'liblinear', \n",
    "        'max_iter': 1000, \n",
    "        'random_state': RANDOM_STATE\n",
    "    }\n",
    "    \n",
    "    # Δημιουργία output directories\n",
    "    script_dir = os.path.dirname(os.path.abspath(''))\n",
    "    experiment_output_base_dir = os.path.join(script_dir, \"outputs\", base_run_id)\n",
    "    reports_output_dir = os.path.join(experiment_output_base_dir, \"reports\")\n",
    "    feature_models_output_dir = os.path.join(experiment_output_base_dir, \"feature_models\")\n",
    "    \n",
    "    # Δημιουργία directories αν δεν υπάρχουν\n",
    "    os.makedirs(reports_output_dir, exist_ok=True)\n",
    "    os.makedirs(feature_models_output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Output directory: {experiment_output_base_dir}\")\n",
    "    \n",
    "    # Αν υπάρχει η run_experiment από utils, χρησιμοποιούμε αυτή\n",
    "    if 'run_experiment' in globals() and all_texts is not None:\n",
    "        try:\n",
    "            run_experiment(\n",
    "                all_texts=all_texts,\n",
    "                all_labels=all_labels,\n",
    "                label_names=label_names,\n",
    "                num_classes=num_classes,\n",
    "                feature_config=feature_config,\n",
    "                model_class=model_class,\n",
    "                model_init_params=model_init_params,\n",
    "                n_splits_cv=N_SPLITS_CV,\n",
    "                single_split_test_size=SINGLE_SPLIT_TEST_SIZE,\n",
    "                random_state=RANDOM_STATE,\n",
    "                reports_output_dir=reports_output_dir,\n",
    "                feature_models_output_dir=feature_models_output_dir,\n",
    "                save_trained_feature_models=SAVE_TRAINED_FEATURE_MODELS,\n",
    "                load_trained_feature_models_if_exist=LOAD_TRAINED_FEATURE_MODELS_IF_EXIST\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error in run_experiment: {e}\")\n",
    "            print(\"Εκτέλεση απλουστευμένου πειράματος...\")\n",
    "            run_simple_word2vec_experiment(feature_config, model_class, model_init_params)\n",
    "    else:\n",
    "        print(\"Εκτέλεση απλουστευμένου πειράματος...\")\n",
    "        run_simple_word2vec_experiment(feature_config, model_class, model_init_params)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"=== Ολοκληρώθηκε το πείραμα σε {end_time - start_time:.2f} δευτερόλεπτα ===\")\n",
    "\n",
    "# Εκτέλεση πειράματος\n",
    "if all_texts is not None:\n",
    "    run_logistic_regression_word2vec_experiment()\n",
    "else:\n",
    "    print(\"Δεν μπορεί να εκτελεστεί το πείραμα - δεν φορτώθηκαν δεδομένα\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21ae1dc",
   "metadata": {},
   "source": [
    "## Απλουστευμένη Εκτέλεση Πειράματος\n",
    "\n",
    "Στην περίπτωση που δεν είναι διαθέσιμες οι συναρτήσεις utils, εκτελείται μια απλουστευμένη έκδοση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617b8f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text):\n",
    "    \"\"\"Απλή tokenization για Word2Vec\"\"\"\n",
    "    return text.lower().split()\n",
    "\n",
    "def train_word2vec_model(texts, params):\n",
    "    \"\"\"Εκπαίδευση Word2Vec μοντέλου\"\"\"\n",
    "    print(\"Tokenization κειμένων...\")\n",
    "    tokenized_texts = [simple_tokenize(text) for text in texts]\n",
    "    \n",
    "    print(\"Εκπαίδευση Word2Vec μοντέλου...\")\n",
    "    model = Word2Vec(\n",
    "        sentences=tokenized_texts,\n",
    "        vector_size=params['vector_size'],\n",
    "        window=params['window'],\n",
    "        min_count=params['min_count'],\n",
    "        workers=params['workers'],\n",
    "        sg=params['sg'],\n",
    "        epochs=params['epochs']\n",
    "    )\n",
    "    \n",
    "    print(f\"Word2Vec vocabulary size: {len(model.wv.key_to_index)}\")\n",
    "    return model\n",
    "\n",
    "def document_to_vector(doc, w2v_model, vector_size):\n",
    "    \"\"\"Μετατροπή εγγράφου σε vector μέσω mean pooling\"\"\"\n",
    "    tokens = simple_tokenize(doc)\n",
    "    word_vectors = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in w2v_model.wv:\n",
    "            word_vectors.append(w2v_model.wv[token])\n",
    "    \n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        # Αν δεν υπάρχουν γνωστές λέξεις, επιστροφή μηδενικού vector\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "def run_simple_word2vec_experiment(feature_config, model_class, model_init_params):\n",
    "    \"\"\"Απλουστευμένη εκτέλεση πειράματος χωρίς utils\"\"\"\n",
    "    \n",
    "    if all_texts is None or all_labels is None:\n",
    "        print(\"Δεν είναι διαθέσιμα δεδομένα για το πείραμα\")\n",
    "        return\n",
    "    \n",
    "    print(\"Εκτέλεση απλουστευμένου πειράματος Word2Vec\")\n",
    "    \n",
    "    # Διαχωρισμός σε training/testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        all_texts, all_labels, \n",
    "        test_size=SINGLE_SPLIT_TEST_SIZE, \n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=all_labels\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Testing samples: {len(X_test)}\")\n",
    "    \n",
    "    # Εκπαίδευση Word2Vec\n",
    "    w2v_params = feature_config['params']\n",
    "    w2v_model = train_word2vec_model(X_train, w2v_params)\n",
    "    \n",
    "    # Μετατροπή εγγράφων σε vectors\n",
    "    print(\"Μετατροπή training documents σε vectors...\")\n",
    "    X_train_vectors = np.array([\n",
    "        document_to_vector(doc, w2v_model, w2v_params['vector_size']) \n",
    "        for doc in X_train\n",
    "    ])\n",
    "    \n",
    "    print(\"Μετατροπή testing documents σε vectors...\")\n",
    "    X_test_vectors = np.array([\n",
    "        document_to_vector(doc, w2v_model, w2v_params['vector_size']) \n",
    "        for doc in X_test\n",
    "    ])\n",
    "    \n",
    "    print(f\"Feature shape: {X_train_vectors.shape}\")\n",
    "    \n",
    "    # Model training\n",
    "    print(\"Εκπαίδευση Logistic Regression μοντέλου...\")\n",
    "    model = model_class(**model_init_params)\n",
    "    model.fit(X_train_vectors, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    print(\"Υπολογισμός προβλέψεων...\")\n",
    "    y_pred = model.predict(X_test_vectors)\n",
    "    \n",
    "    # Evaluation\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, target_names=label_names)\n",
    "    \n",
    "    print(f\"\\n=== Αποτελέσματα για Logistic Regression + Word2Vec ===\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Επιπλέον πληροφορίες\n",
    "    print(f\"\\nWord2Vec Info:\")\n",
    "    print(f\"- Vocabulary size: {len(w2v_model.wv.key_to_index)}\")\n",
    "    print(f\"- Vector dimensions: {w2v_params['vector_size']}\")\n",
    "    print(f\"- Training algorithm: {'Skip-gram' if w2v_params['sg'] else 'CBOW'}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'classification_report': report,\n",
    "        'w2v_vocab_size': len(w2v_model.wv.key_to_index),\n",
    "        'method': 'LogisticRegression + Word2Vec'\n",
    "    }\n",
    "\n",
    "print(\"Συνάρτηση απλουστευμένου πειράματος ορίστηκε επιτυχώς\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6472e0b4",
   "metadata": {},
   "source": [
    "## Εντολές Εκτέλεσης\n",
    "\n",
    "Για να εκτελέσετε το πείραμα χωριστά μέσω terminal, χρησιμοποιήστε:\n",
    "\n",
    "```bash\n",
    "# Για Logistic Regression με Word2Vec\n",
    "python merosB1/ii/main_LogRegr_W2V.py\n",
    "```\n",
    "\n",
    "Τα αποτελέσματα θα αποθηκευτούν στον φάκελο `outputs/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc77ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Εναλλακτική εκτέλεση με υποσύνολο δεδομένων για γρήγορη δοκιμή\n",
    "print(\"\\n=== Γρήγορη Δοκιμή με Υποσύνολο Δεδομένων ===\")\n",
    "\n",
    "if all_texts is not None and len(all_texts) > 500:\n",
    "    # Χρήση 5% των δεδομένων για γρήγορη δοκιμή\n",
    "    subset_size = max(100, int(len(all_texts) * 0.05))  # Τουλάχιστον 100 samples\n",
    "    indices = np.random.choice(len(all_texts), subset_size, replace=False)\n",
    "    \n",
    "    subset_texts = [all_texts[i] for i in indices]\n",
    "    subset_labels = all_labels[indices]\n",
    "    \n",
    "    print(f\"Χρήση {subset_size} samples από {len(all_texts)} για γρήγορη δοκιμή\")\n",
    "    \n",
    "    # Διαχωρισμός δεδομένων\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        subset_texts, subset_labels, test_size=0.2, random_state=42, stratify=subset_labels\n",
    "    )\n",
    "    \n",
    "    # Γρήγορη εκπαίδευση Word2Vec με μικρότερες παραμέτρους\n",
    "    quick_w2v_params = {\n",
    "        'vector_size': 50,   # Μικρότερο vector size\n",
    "        'window': 3,         # Μικρότερο window\n",
    "        'min_count': 1,      # Χαμηλότερο min_count\n",
    "        'workers': 2,        # Λιγότεροι workers\n",
    "        'sg': 0,             # CBOW\n",
    "        'epochs': 5          # Λιγότερες εποχές\n",
    "    }\n",
    "    \n",
    "    print(\"Γρήγορη εκπαίδευση Word2Vec...\")\n",
    "    w2v_model = train_word2vec_model(X_train, quick_w2v_params)\n",
    "    \n",
    "    # Feature extraction\n",
    "    X_train_vectors = np.array([\n",
    "        document_to_vector(doc, w2v_model, quick_w2v_params['vector_size']) \n",
    "        for doc in X_train\n",
    "    ])\n",
    "    X_test_vectors = np.array([\n",
    "        document_to_vector(doc, w2v_model, quick_w2v_params['vector_size']) \n",
    "        for doc in X_test\n",
    "    ])\n",
    "    \n",
    "    # Quick training\n",
    "    lr_model = LogisticRegression(solver='liblinear', max_iter=500, random_state=42)\n",
    "    lr_model.fit(X_train_vectors, y_train)\n",
    "    \n",
    "    y_pred = lr_model.predict(X_test_vectors)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nΓρήγορη δοκιμή - Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Word2Vec vocabulary: {len(w2v_model.wv.key_to_index)} λέξεις\")\n",
    "    print(f\"Feature dimensions: {quick_w2v_params['vector_size']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Δεν υπάρχουν αρκετά δεδομένα για δοκιμή\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4572e4",
   "metadata": {},
   "source": [
    "## Σύγκριση με BoW/TF-IDF Μεθόδους\n",
    "\n",
    "### Word2Vec vs Traditional Methods:\n",
    "\n",
    "| Κριτήριο | Word2Vec | BoW/TF-IDF |\n",
    "|----------|----------|------------|\n",
    "| **Διάσταση Features** | Χαμηλή (100-300) | Υψηλή (χιλιάδες) |\n",
    "| **Σημασιολογική Πληροφορία** | Ναι | Όχι |\n",
    "| **Συχνότητα Λέξεων** | Όχι άμεσα | Ναι |\n",
    "| **Αραιότητα** | Dense vectors | Sparse vectors |\n",
    "| **Υπολογιστική Πολυπλοκότητα** | Μέτρια (training) | Χαμηλή |\n",
    "| **Generalization** | Καλή | Περιορισμένη |\n",
    "| **Interpretability** | Μέτρια | Υψηλή |\n",
    "\n",
    "### Αναμενόμενα Αποτελέσματα:\n",
    "\n",
    "**Word2Vec αναμένεται να υπερτερεί όταν:**\n",
    "- Το corpus έχει πλούσιο vocabulary\n",
    "- Υπάρχουν σημασιολογικές σχέσεις μεταξύ λέξεων\n",
    "- Το dataset είναι αρκετά μεγάλο για καλή εκπαίδευση embeddings\n",
    "\n",
    "**BoW/TF-IDF μπορεί να είναι καλύτερο όταν:**\n",
    "- Η συχνότητα λέξεων είναι κρίσιμη\n",
    "- Το dataset είναι μικρό\n",
    "- Χρειάζεται γρήγορη εκτέλεση\n",
    "\n",
    "### Βελτιστοποιήσεις Word2Vec:\n",
    "\n",
    "1. **Hyperparameter Tuning:**\n",
    "   - Vector size: δοκιμή 100, 200, 300\n",
    "   - Window size: 3-10 ανάλογα με το είδος κειμένου\n",
    "   - Min count: ισορροπία μεταξύ vocabulary size και coverage\n",
    "\n",
    "2. **Preprocessing:**\n",
    "   - Καλύτερη tokenization (αφαίρεση σημείων στίξης)\n",
    "   - Stemming/Lemmatization για ελληνικά\n",
    "   - Αφαίρεση stop words\n",
    "\n",
    "3. **Aggregation Methods:**\n",
    "   - Weighted average με TF-IDF weights\n",
    "   - Doc2Vec για document-level embeddings\n",
    "   - Attention-based pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adb991f",
   "metadata": {},
   "source": [
    "## Πρακτικές Συμβουλές για Βελτίωση\n",
    "\n",
    "### Προεπεξεργασία Κειμένου:\n",
    "- **Tokenization**: Χρήση πιο εξελιγμένων tokenizers\n",
    "- **Normalization**: Μετατροπή σε πεζά, αφαίρεση ειδικών χαρακτήρων\n",
    "- **Stop Words**: Αφαίρεση συχνών λέξεων χωρίς σημασιολογική αξία\n",
    "- **Stemming**: Μείωση λέξεων στη ρίζα τους (για ελληνικά: greek stemmer)\n",
    "\n",
    "### Βελτιστοποίηση Word2Vec:\n",
    "- **Vector Size**: Μεγαλύτερα vectors για πλουσιότερη αναπαράσταση\n",
    "- **Training Data**: Περισσότερα δεδομένα για καλύτερα embeddings\n",
    "- **Pre-trained Embeddings**: Χρήση προ-εκπαιδευμένων Greek embeddings\n",
    "- **Sub-word Models**: FastText για handling OOV words\n",
    "\n",
    "### Μοντέλο Optimization:\n",
    "- **Regularization**: L1/L2 για αποφυγή overfitting\n",
    "- **Cross-validation**: K-fold για αξιόπιστα αποτελέσματα\n",
    "- **Feature Engineering**: Συνδυασμός Word2Vec με άλλα features\n",
    "- **Ensemble Methods**: Συνδυασμός με άλλους classifiers\n",
    "\n",
    "### Αξιολόγηση:\n",
    "- **Confusion Matrix**: Ανάλυση λαθών ανά κατηγορία\n",
    "- **Feature Analysis**: Εύρεση των πιο σημαντικών dimensions\n",
    "- **Learning Curves**: Έλεγχος για overfitting/underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73080741",
   "metadata": {},
   "source": [
    "## Αναμενόμενα Αποτελέσματα βάσει των προηγούμενων εκτελέσεων\n",
    "\n",
    "Βάσει των αρχείων στο `outputsfinal/ii/`, αναμένουμε:\n",
    "\n",
    "- **Γενικά καλή απόδοση** σε σχέση με BoW/TF-IDF μεθόδους\n",
    "- **Βελτίωση στη generalization** λόγω semantic embeddings\n",
    "- **Καλύτερα αποτελέσματα σε `subject` configuration** \n",
    "- **Πιθανή υστέρηση σε μικρά datasets** λόγω της πολυπλοκότητας του Word2Vec\n",
    "- **Trade-off μεταξύ ταχύτητας και απόδοσης** σε σχέση με απλούστερες μεθόδους\n",
    "\n",
    "---\n",
    "\n",
    "## Σύνοψη Αποτελεσμάτων\n",
    "\n",
    "Τα αποτελέσματα του πειράματος θα αποθηκευτούν στον φάκελο `outputs/` και θα περιλαμβάνουν:\n",
    "\n",
    "1. **Logistic Regression με Word2Vec**: Classification reports και μετρικές απόδοσης\n",
    "2. **Εκπαιδευμένο Word2Vec μοντέλο**: Για μελλοντική χρήση\n",
    "3. **Feature vectors**: Document-level representations\n",
    "\n",
    "Κάθε πείραμα παράγει:\n",
    "- Classification report (σε μορφή JSON και text)\n",
    "- Αποθηκευμένα μοντέλα (Word2Vec και Logistic Regression)\n",
    "- Λεπτομερή logs της εκτέλεσης\n",
    "- Στατιστικά για το Word2Vec vocabulary\n",
    "\n",
    "Η σύγκριση με τις μεθόδους του Β1.i θα δείξει τα πλεονεκτήματα και μειονεκτήματα των semantic embeddings έναντι των frequency-based μεθόδων."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
